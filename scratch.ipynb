{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f54d15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DataFolio Demo - Protein Analysis Workflow\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datafolio import DataFolio\n",
    "\n",
    "# Clean up any existing test bundles\n",
    "import shutil\n",
    "if Path('demo_bundles').exists():\n",
    "    shutil.rmtree('demo_bundles')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DataFolio Demo - Protein Analysis Workflow\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f55f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä STEP 1: Creating simulated external datasets...\n",
      "  ‚úì Created external training data: 1000 proteins\n",
      "  ‚úì Created external validation data: 200 proteins\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä STEP 1: Creating simulated external datasets...\")\n",
    "\n",
    "# Create temp directory for external data\n",
    "external_data_dir = Path('temp_external_data')\n",
    "external_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Large training dataset (simulated datalake data)\n",
    "training_data = pd.DataFrame({\n",
    "    'protein_id': [f'PROT_{i:05d}' for i in range(1000)],\n",
    "    'sequence_length': np.random.randint(50, 500, 1000),\n",
    "    'hydrophobicity': np.random.randn(1000),\n",
    "    'charge': np.random.randn(1000),\n",
    "    'label': np.random.choice(['membrane', 'cytoplasmic', 'nuclear'], 1000)\n",
    "})\n",
    "training_file = external_data_dir / 'training_proteins.parquet'\n",
    "training_data.to_parquet(training_file, index=False)\n",
    "print(f\"  ‚úì Created external training data: {len(training_data)} proteins\")\n",
    "\n",
    "# Validation dataset\n",
    "validation_data = pd.DataFrame({\n",
    "    'protein_id': [f'PROT_{i:05d}' for i in range(1000, 1200)],\n",
    "    'sequence_length': np.random.randint(50, 500, 200),\n",
    "    'hydrophobicity': np.random.randn(200),\n",
    "    'charge': np.random.randn(200),\n",
    "    'label': np.random.choice(['membrane', 'cytoplasmic', 'nuclear'], 200)\n",
    "})\n",
    "validation_file = external_data_dir / 'validation_proteins.parquet'\n",
    "validation_data.to_parquet(validation_file, index=False)\n",
    "print(f\"  ‚úì Created external validation data: {len(validation_data)} proteins\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ac800a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üóÇÔ∏è  STEP 2: Creating DataFolio bundle...\n",
      "  ‚úì Bundle created: demo_bundles/protein-analysis-mindful-indigo-viper\n",
      "  ‚úì Initial metadata keys: ['experiment', 'date', 'scientist', 'model_type', 'description', 'parameters', 'created_at', 'updated_at']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüóÇÔ∏è  STEP 2: Creating DataFolio bundle...\")\n",
    "\n",
    "folio = DataFolio(\n",
    "    path='demo_bundles',\n",
    "    prefix='protein-analysis',\n",
    "    metadata={\n",
    "        'experiment': 'protein_localization_v2',\n",
    "        'date': '2024-01-15',\n",
    "        'scientist': 'Dr. Smith',\n",
    "        'model_type': 'random_forest',\n",
    "        'description': 'Protein subcellular localization prediction',\n",
    "        'parameters': {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 10,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"  ‚úì Bundle created: {folio._bundle_dir}\")\n",
    "print(f\"  ‚úì Initial metadata keys: {list(folio.metadata.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a51eedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó STEP 3: Referencing external datasets...\n",
      "  ‚úì Referenced training data (1000 rows)\n",
      "  ‚úì Referenced validation data (200 rows)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîó STEP 3: Referencing external datasets...\")\n",
    "\n",
    "folio.reference_table(\n",
    "    'training_data',\n",
    "    path=str(training_file),\n",
    "    table_format='parquet',\n",
    "    num_rows=len(training_data),\n",
    "    description='Large training dataset from protein database',\n",
    "    code='training_data.to_parquet(training_file)'\n",
    ")\n",
    "\n",
    "folio.reference_table(\n",
    "    'validation_data',\n",
    "    path=str(validation_file),\n",
    "    table_format='parquet',\n",
    "    num_rows=len(validation_data),\n",
    "    description='Validation dataset for model evaluation',\n",
    "    code='validation_data.to_parquet(validation_file)'\n",
    ")\n",
    "\n",
    "print(f\"  ‚úì Referenced training data ({len(training_data)} rows)\")\n",
    "print(f\"  ‚úì Referenced validation data ({len(validation_data)} rows)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0165155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà STEP 4: Adding analysis results...\n",
      "  ‚úì Added performance metrics\n",
      "  ‚úì Added confusion matrix\n",
      "  ‚úì Added feature importance\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìà STEP 4: Adding analysis results...\")\n",
    "\n",
    "# Model performance metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'metric': ['accuracy', 'precision', 'recall', 'f1_score'],\n",
    "    'train': [0.945, 0.932, 0.928, 0.930],\n",
    "    'validation': [0.912, 0.901, 0.898, 0.899]\n",
    "})\n",
    "folio.add_table(\n",
    "    'performance_metrics',\n",
    "    metrics_df,\n",
    "    description='Model performance summary',\n",
    "    inputs=['training_data', 'validation_data'],\n",
    "    models=['rf_classifier'],\n",
    "    code='evaluate_model(model, X_train, y_train, X_val, y_val)'\n",
    ")\n",
    "print(f\"  ‚úì Added performance metrics\")\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix = pd.DataFrame(\n",
    "    [[150, 10, 5],\n",
    "     [8, 140, 12],\n",
    "     [7, 15, 138]],\n",
    "    index=['membrane', 'cytoplasmic', 'nuclear'],\n",
    "    columns=['pred_membrane', 'pred_cytoplasmic', 'pred_nuclear']\n",
    ")\n",
    "folio.add_table(\n",
    "    'confusion_matrix',\n",
    "    confusion_matrix,\n",
    "    description='Validation confusion matrix',\n",
    "    inputs=['validation_data'],\n",
    "    models=['rf_classifier'],\n",
    "    code='confusion_matrix(y_true, y_pred)'\n",
    ")\n",
    "print(f\"  ‚úì Added confusion matrix\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': ['sequence_length', 'hydrophobicity', 'charge'],\n",
    "    'importance': [0.45, 0.35, 0.20]\n",
    "}).sort_values('importance', ascending=False)\n",
    "folio.add_table(\n",
    "    'feature_importance',\n",
    "    feature_importance,\n",
    "    description='RF feature importance',\n",
    "    models=['rf_classifier'],\n",
    "    code='pd.DataFrame(model.feature_importances_)'\n",
    ")\n",
    "print(f\"  ‚úì Added feature importance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7f66866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ STEP 5: Adding trained model...\n",
      "  ‚úì Model trained and added to bundle\n",
      "  ‚úì Scaler added to bundle\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nü§ñ STEP 5: Adding trained model...\")\n",
    "\n",
    "# Create and \"train\" a simple model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare features\n",
    "X_train = training_data[['sequence_length', 'hydrophobicity', 'charge']].values\n",
    "y_train = training_data['label'].values\n",
    "\n",
    "# Train model\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Add model to bundle\n",
    "folio.add_model(\n",
    "    'rf_classifier',\n",
    "    model,\n",
    "    description='Random forest classifier for protein localization',\n",
    "    inputs=['training_data'],\n",
    "    hyperparameters={'n_estimators': 100, 'max_depth': 10, 'random_state': 42},\n",
    "    code='model = RandomForestClassifier(...).fit(X_train_scaled, y_train)'\n",
    ")\n",
    "print(f\"  ‚úì Model trained and added to bundle\")\n",
    "\n",
    "# Also save the scaler\n",
    "folio.add_model(\n",
    "    'scaler',\n",
    "    scaler,\n",
    "    description='Feature scaler (StandardScaler)',\n",
    "    inputs=['training_data'],\n",
    "    code='scaler = StandardScaler().fit(X_train)'\n",
    ")\n",
    "print(f\"  ‚úì Scaler added to bundle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16781650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé® STEP 6: Adding artifacts...\n",
      "  ‚úì Added feature importance plot\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüé® STEP 6: Adding artifacts...\")\n",
    "\n",
    "# Create a simple plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature importance plot\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "feature_importance.plot(x='feature', y='importance', kind='barh', ax=ax, legend=False)\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plot_file = Path('temp_plot.png')\n",
    "plt.savefig(plot_file)\n",
    "plt.close()\n",
    "\n",
    "folio.add_artifact('feature_importance_plot', plot_file, category='plots', description='Feature importance visualization')\n",
    "plot_file.unlink()  # Clean up temp file\n",
    "print(f\"  ‚úì Added feature importance plot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a2fb70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úèÔ∏è  STEP 7: Updating metadata...\n",
      "  ‚úì Metadata updated (auto-saved!)\n",
      "  ‚úì Total metadata keys: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n‚úèÔ∏è  STEP 7: Updating metadata...\")\n",
    "\n",
    "folio.metadata['training_samples'] = len(training_data)\n",
    "folio.metadata['validation_samples'] = len(validation_data)\n",
    "folio.metadata['final_accuracy'] = 0.912\n",
    "folio.metadata['notes'] = 'Initial model performs well, consider adding more features'\n",
    "\n",
    "print(f\"  ‚úì Metadata updated (auto-saved!)\")\n",
    "print(f\"  ‚úì Total metadata keys: {len(folio.metadata)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad2c3cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ STEP 9: Loading existing bundle (simulating new session)...\n",
      "  ‚úì Loaded bundle from: demo_bundles/protein-analysis-mindful-indigo-viper\n",
      "  ‚úì Experiment: protein_localization_v2\n",
      "  ‚úì Final accuracy: 0.912\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìÇ STEP 9: Loading existing bundle (simulating new session)...\")\n",
    "\n",
    "# Get the bundle directory path\n",
    "bundle_path = folio._bundle_dir\n",
    "\n",
    "# Load it\n",
    "loaded_folio = DataFolio(path=bundle_path)\n",
    "\n",
    "print(f\"  ‚úì Loaded bundle from: {bundle_path}\")\n",
    "print(f\"  ‚úì Experiment: {loaded_folio.metadata['experiment']}\")\n",
    "print(f\"  ‚úì Final accuracy: {loaded_folio.metadata['final_accuracy']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d532fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFolio: demo_bundles/protein-analysis-mindful-indigo-viper\n",
      "=============================================================\n",
      "\n",
      "Created: 2025-10-13T16:18:39.741317+00:00\n",
      "Updated: 2025-10-13T16:19:30.600211+00:00\n",
      "\n",
      "Referenced Tables (2):\n",
      "  ‚Ä¢ training_data [referenced_table]: Large training dataset from protein database\n",
      "  ‚Ä¢ validation_data [referenced_table]: Validation dataset for model evaluation\n",
      "\n",
      "Included Tables (3):\n",
      "  ‚Ä¢ performance_metrics [included_table]: Model performance summary\n",
      "    ‚Ü≥ inputs: training_data, validation_data\n",
      "    ‚Ü≥ models: rf_classifier\n",
      "  ‚Ä¢ confusion_matrix [included_table]: Validation confusion matrix\n",
      "    ‚Ü≥ inputs: validation_data\n",
      "    ‚Ü≥ models: rf_classifier\n",
      "  ‚Ä¢ feature_importance [included_table]: RF feature importance\n",
      "    ‚Ü≥ models: rf_classifier\n",
      "\n",
      "Models (2):\n",
      "  ‚Ä¢ rf_classifier [model]: Random forest classifier for protein localization\n",
      "    ‚Ü≥ inputs: training_data\n",
      "    ‚Ü≥ hyperparameters: n_estimators=100, max_depth=10, random_state=42\n",
      "  ‚Ä¢ scaler [model]: Feature scaler (StandardScaler)\n",
      "    ‚Ü≥ inputs: training_data\n",
      "\n",
      "Artifacts (1):\n",
      "  ‚Ä¢ feature_importance_plot [artifact] (plots): Feature importance visualization\n"
     ]
    }
   ],
   "source": [
    "print(loaded_folio.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad85eb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ‚úì Reading referenced training data...\n",
      "    Shape: (1000, 5)\n",
      "    First 3 rows:\n"
     ]
    }
   ],
   "source": [
    "# Read referenced table (from external file)\n",
    "print(f\"\\n  ‚úì Reading referenced training data...\")\n",
    "training = loaded_folio.get_table('training_data')\n",
    "print(f\"    Shape: {training.shape}\")\n",
    "print(f\"    First 3 rows:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abecb5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = loaded_folio.get_model('rf_classifier')\n",
    "loaded_scaler = loaded_folio.get_model('scaler')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0fb3e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sample prediction: nuclear\n",
      "    Actual label: cytoplasmic\n"
     ]
    }
   ],
   "source": [
    "sample = validation_data.iloc[0:1][['sequence_length', 'hydrophobicity', 'charge']].values\n",
    "sample_scaled = loaded_scaler.transform(sample)\n",
    "prediction = loaded_model.predict(sample_scaled)\n",
    "print(f\"    Sample prediction: {prediction[0]}\")\n",
    "print(f\"    Actual label: {validation_data.iloc[0]['label']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d00953bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó STEP 11: Exploring lineage tracking...\n",
      "\n",
      "DataFolio: demo_bundles/protein-analysis-mindful-indigo-viper\n",
      "=============================================================\n",
      "\n",
      "Created: 2025-10-13T16:18:39.741317+00:00\n",
      "Updated: 2025-10-13T16:19:30.600211+00:00\n",
      "\n",
      "Referenced Tables (2):\n",
      "  ‚Ä¢ training_data [referenced_table]: Large training dataset from protein database\n",
      "  ‚Ä¢ validation_data [referenced_table]: Validation dataset for model evaluation\n",
      "\n",
      "Included Tables (3):\n",
      "  ‚Ä¢ performance_metrics [included_table]: Model performance summary\n",
      "    ‚Ü≥ inputs: training_data, validation_data\n",
      "    ‚Ü≥ models: rf_classifier\n",
      "  ‚Ä¢ confusion_matrix [included_table]: Validation confusion matrix\n",
      "    ‚Ü≥ inputs: validation_data\n",
      "    ‚Ü≥ models: rf_classifier\n",
      "  ‚Ä¢ feature_importance [included_table]: RF feature importance\n",
      "    ‚Ü≥ models: rf_classifier\n",
      "\n",
      "Models (2):\n",
      "  ‚Ä¢ rf_classifier [model]: Random forest classifier for protein localization\n",
      "    ‚Ü≥ inputs: training_data\n",
      "    ‚Ü≥ hyperparameters: n_estimators=100, max_depth=10, random_state=42\n",
      "  ‚Ä¢ scaler [model]: Feature scaler (StandardScaler)\n",
      "    ‚Ü≥ inputs: training_data\n",
      "\n",
      "Artifacts (1):\n",
      "  ‚Ä¢ feature_importance_plot [artifact] (plots): Feature importance visualization\n",
      "\n",
      "  Lineage queries:\n",
      "    performance_metrics inputs: ['training_data', 'validation_data', 'rf_classifier']\n",
      "    training_data dependents: ['performance_metrics', 'rf_classifier', 'scaler']\n",
      "    rf_classifier dependents: ['performance_metrics', 'confusion_matrix', 'feature_importance']\n",
      "\n",
      "  Full dependency graph:\n",
      "    performance_metrics ‚Üê ['training_data', 'validation_data', 'rf_classifier']\n",
      "    confusion_matrix ‚Üê ['validation_data', 'rf_classifier']\n",
      "    feature_importance ‚Üê ['rf_classifier']\n",
      "    rf_classifier ‚Üê ['training_data']\n",
      "    scaler ‚Üê ['training_data']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîó STEP 11: Exploring lineage tracking...\")\n",
    "\n",
    "# View bundle description with lineage\n",
    "print(\"\\n\" + folio.describe())\n",
    "\n",
    "# Query lineage relationships\n",
    "print(\"\\n  Lineage queries:\")\n",
    "print(f\"    performance_metrics inputs: {folio.get_inputs('performance_metrics')}\")\n",
    "print(f\"    training_data dependents: {folio.get_dependents('training_data')}\")\n",
    "print(f\"    rf_classifier dependents: {folio.get_dependents('rf_classifier')}\")\n",
    "\n",
    "# View full lineage graph\n",
    "graph = folio.get_lineage_graph()\n",
    "print(\"\\n  Full dependency graph:\")\n",
    "for item, inputs in graph.items():\n",
    "    if inputs:\n",
    "        print(f\"    {item} ‚Üê {inputs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19cadad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "folio_tuned = folio.copy(\n",
    "    new_path='demo_bundles',\n",
    "    new_prefix='protein-analysis-tuned',\n",
    "    metadata_updates={\n",
    "        'experiment': 'protein_localization_v2_tuned',\n",
    "        'parent_experiment': folio.metadata['experiment'],\n",
    "        'changes': 'Hyperparameter tuning variant',\n",
    "    },\n",
    "    exclude_items=['performance_metrics', 'confusion_matrix', 'feature_importance']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ba6f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Created tuned variant: demo_bundles/protein-analysis-tuned-keen-indigo-ibis\n",
      "    Items: ['training_data', 'validation_data', 'feature_importance_plot', 'scaler', 'rf_classifier']\n",
      "    Note: Excluded metrics - will regenerate after retraining\n"
     ]
    }
   ],
   "source": [
    "print(f\"  ‚úì Created tuned variant: {folio_tuned._bundle_dir}\")\n",
    "print(f\"    Items: {list(folio_tuned._items.keys())}\")\n",
    "print(f\"    Note: Excluded metrics - will regenerate after retraining\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9929baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datafolio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
